# Environment Configuration for Vector Code Retrieval System

# Ollama Configuration
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=qwen3:8b

# Embedding Configuration
EMBEDDING_SERVER=http://localhost:5000
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# Storage Configuration
CHROMA_PATH=./chroma_code

# Service Selection
USE_LOCAL_EMBEDDINGS=true
USE_LOCAL_OLLAMA=true

# Trust Remote Code Settings (automatically managed by trust_manager.py)
# Format: TRUST_REMOTE_CODE_<MODEL_HASH>=true|false
# These are automatically added when you approve/deny models
# Example:
# # TRUST_REMOTE_CODE_A1B2C3D4_MODEL=nomic-ai/nomic-embed-text-v1.5
# TRUST_REMOTE_CODE_A1B2C3D4=true