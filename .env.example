# Environment Configuration for Vector Code Retrieval System

# Ollama Configuration
OLLAMA_HOST=http://127.0.0.1:11434
OLLAMA_MODEL=qwen3:8b
OLLAMA_EMBEDDING_MODEL=nomic-embed-text:latest

# Embedding Configuration
EMBEDDING_SERVER=http://127.0.0.1:5000
EMBEDDING_MODEL=nomic-ai/nomic-embed-text-v1.5

# Storage Configuration
CHROMA_PATH=./chroma_db
DEFAULT_CHUNK_SIZE=2000

# Query Configuration
DEFAULT_OUTPUT_FILE_PREFIX=ask
DEFAULT_TOP_K=5

# Service Selection
USE_LOCAL_EMBEDDINGS=true
USE_LOCAL_OLLAMA=true

# Trust Remote Code Settings (automatically managed by trust_manager.py)
# Format: TRUST_REMOTE_CODE_<MODEL_HASH>=true|false
# These are automatically added when you approve/deny models
# Example:
# # TRUST_REMOTE_CODE_A1B2C3D4_MODEL=nomic-ai/nomic-embed-text-v1.5
# TRUST_REMOTE_CODE_A1B2C3D4=true# TRUST_REMOTE_CODE_44A32A10_MODEL=nomic-ai/nomic-embed-text-v1.5